[0m[[0m[0mdebug[0m] [0m[0m> Exec(run, Some(83178a3d-856d-432c-85b9-49dad22cd3cc), Some(CommandSource(console0)))[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / run[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[31merror[0m] [0m[0mjava.net.ConnectException: Call From Zens-MacBook-Air.local/127.0.0.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client.call(Client.java:1519)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client.call(Client.java:1416)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.sun.proxy.$Proxy32.getFileInfo(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.sun.proxy.$Proxy33.getFileInfo(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1731)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1752)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1749)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1764)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.Globber.glob(Globber.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:276)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1056)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1053)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:960)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:955)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.net.ConnectException: Connection refused[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:822)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client.call(Client.java:1463)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.Client.call(Client.java:1416)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.sun.proxy.$Proxy32.getFileInfo(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.sun.proxy.$Proxy33.getFileInfo(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1731)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1752)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1749)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1764)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.Globber.glob(Globber.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:276)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1056)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1053)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:960)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:955)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) java.net.ConnectException: Call From Zens-MacBook-Air.local/127.0.0.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused[0m
[0m[[0m[31merror[0m] [0m[0mTotal time: 76 s (01:16), completed Feb 1, 2022 7:42:08 PM[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0minfo[0m] [0m[0mshutting down sbt server[0m
